"""Data7 implementation benchmark.

Our main intention here is to compare data7 initial implementation using
the CSV native library against Pandas.

You can run this script with the following command:

poetry run python scripts/benchmark.py

For performance testing, we use the imdb-sql project to seed the database:
https://github.com/jmaupetit/imdb-sql

"""

import asyncio
import csv
from io import StringIO
from typing import Any, AsyncGenerator

import databases
from data7.app import Dataset
from data7.app import sql2csv as pd_sql2csv
from data7.config import settings
from pyinstrument import Profiler
from rich.console import Console
from rich.live import Live
from rich.pretty import Pretty, pprint
from rich.table import Table
from sqlalchemy import create_engine

# Databases
database = databases.Database("postgresql+asyncpg://imdb:pass@localhost:5432/imdb")
engine = create_engine("postgresql://imdb:pass@localhost:5432/imdb")

# Profiling
aprofiler = Profiler(interval=settings.profiler_interval, async_mode="enabled")
profiler = Profiler(interval=settings.profiler_interval, async_mode="disabled")

# Display
console = Console()


title_fields = [
    "tconst",
    "titleType",
    "primaryTitle",
    "originalTitle",
    "isAdult",
    "startYear",
    "endYear",
    "runtimeMinutes",
    "genres",
]


# Legacy
async def sql2csv(dataset: Dataset) -> AsyncGenerator[str, Any]:
    """Stream SQL rows to CSV."""
    output = StringIO()
    writer = csv.DictWriter(output, fieldnames=title_fields)

    # Header
    writer.writeheader()
    output.seek(0)
    yield output.readline()

    # Rows
    async for record in database.iterate(dataset.query):
        output.seek(0)
        writer.writerow(dict(zip(record.keys(), record.values())))
        output.seek(0)
        yield output.readline()


# Wrappers
async def _sql2csv(dataset: Dataset) -> float:
    """sql2csv wrapper to handle database connection."""
    await database.connect()
    aprofiler.reset()
    aprofiler.start()
    async for _ in sql2csv(dataset):
        pass
    aprofiler.stop()
    await database.disconnect()
    return aprofiler.last_session.duration if aprofiler.last_session else 0.0


def _pd_sql2csv(dataset: Dataset, chunksize: int = 5000) -> float:
    """sql2any wrapper to handled database connection."""
    profiler.reset()
    profiler.start()
    for _ in pd_sql2csv(dataset, chunksize):
        pass
    profiler.stop()
    return profiler.last_session.duration if profiler.last_session else 0.0


# Output
def render_float(value):
    """Pretty print a float."""
    return pprint(value)


table = Table(title="Data7 implementations Benchmark (Duration for CSV)")
table.add_column("# rows")
table.add_column("v0.5.0", justify="right")
table.add_column("üêº (1000)", justify="right")
table.add_column("Ratio (1000)", justify="right")
table.add_column("üêº (5000)", justify="right")
table.add_column("Ratio (5000)", justify="right")
table.add_column("üêº (10000)", justify="right")
table.add_column("Ratio (10000)", justify="right")

# ruff: noqa: S608
datasets = [
    Dataset(
        basename=f"{rows}",
        query=f'SELECT * FROM "title.basics" LIMIT {rows}',
    )
    for rows in (500, 1000, 5000, 10000, 50000, 100000, 500000, 1000000, 5000000)
]

with Live(table, refresh_per_second=4):
    for dataset in datasets:
        # Legacy
        legacy = asyncio.run(_sql2csv(dataset))

        # Pandas
        pandas = []
        ratios = []
        for chunksize in (1000, 5000, 10000):
            p = _pd_sql2csv(dataset, chunksize)
            pandas.append(p)
            ratios.append(legacy / p)
        values = (v for couple in zip(pandas, ratios) for v in couple)

        table.add_row(
            dataset.basename,
            Pretty(round(legacy, 3)),
            *(Pretty(round(v, 3)) for v in values),
        )

engine.dispose()
